{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0def2469",
   "metadata": {},
   "source": [
    "Annotation images → mask label for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1011878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "EDGE_DIR = \"training_set/annos_edge\"\n",
    "MASK_DIR = \"training_set/masks_filled\"\n",
    "os.makedirs(MASK_DIR, exist_ok=True)\n",
    "\n",
    "for fname in os.listdir(EDGE_DIR):\n",
    "    edge = cv2.imread(os.path.join(EDGE_DIR, fname), 0)\n",
    "    _, edge = cv2.threshold(edge, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    contours, _ = cv2.findContours(\n",
    "        edge, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE\n",
    "    )\n",
    "    if len(contours) == 0:\n",
    "        continue\n",
    "\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    filled = np.zeros_like(edge)\n",
    "    cv2.drawContours(filled, [cnt], -1, 255, thickness=-1)\n",
    "\n",
    "    cv2.imwrite(os.path.join(MASK_DIR, fname), filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae947cfb",
   "metadata": {},
   "source": [
    "Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2, pandas as pd, numpy as np\n",
    "\n",
    "IMG_SIZE = 256\n",
    "\n",
    "class FetalDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img = cv2.imread(f\"training_set/images/{row['filename']}\", 0)\n",
    "        mask = cv2.imread(f\"training_set/masks_filled/{row['filename']}\", 0)\n",
    "\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        mask = cv2.resize(mask, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        img = img / 255.0\n",
    "        mask = (mask > 0).astype(np.float32)\n",
    "\n",
    "        img = np.stack([img]*3, axis=0)   # (3,H,W)\n",
    "        mask = mask[None,:,:]             # (1,H,W)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(img, dtype=torch.float32),\n",
    "            torch.tensor(mask, dtype=torch.float32),\n",
    "            row[\"pixel size(mm)\"],\n",
    "            row[\"head circumference (mm)\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad118e1",
   "metadata": {},
   "source": [
    "U-Net pretrained (fine-tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1207d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1\n",
    ").to(DEVICE)\n",
    "dice_loss = smp.losses.DiceLoss(mode=\"binary\")\n",
    "bce_loss = smp.losses.SoftBCEWithLogitsLoss()\n",
    "def loss_fn(pred, target):\n",
    "    return dice_loss(pred, target) + bce_loss(pred, target)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead9216",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3afc7df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loop entered 0\n",
      "Epoch 1: loss=0.2267\n",
      "Epoch loop entered 1\n",
      "Epoch 2: loss=0.1527\n",
      "Epoch loop entered 2\n",
      "Epoch 3: loss=0.1243\n",
      "Epoch loop entered 3\n",
      "Epoch 4: loss=0.0971\n",
      "Epoch loop entered 4\n",
      "Epoch 5: loss=0.0777\n",
      "Epoch loop entered 5\n",
      "Epoch 6: loss=0.0684\n",
      "Epoch loop entered 6\n",
      "Epoch 7: loss=0.0590\n",
      "Epoch loop entered 7\n",
      "Epoch 8: loss=0.0537\n",
      "Epoch loop entered 8\n",
      "Epoch 9: loss=0.0474\n",
      "Epoch loop entered 9\n",
      "Epoch 10: loss=0.0431\n",
      "Epoch loop entered 10\n",
      "Epoch 11: loss=0.0396\n",
      "Epoch loop entered 11\n",
      "Epoch 12: loss=0.0371\n",
      "Epoch loop entered 12\n",
      "Epoch 13: loss=0.0327\n",
      "Epoch loop entered 13\n",
      "Epoch 14: loss=0.0313\n",
      "Epoch loop entered 14\n",
      "Epoch 15: loss=0.0300\n",
      "Epoch loop entered 15\n",
      "Epoch 16: loss=0.0459\n",
      "Epoch loop entered 16\n",
      "Epoch 17: loss=0.0701\n",
      "Epoch loop entered 17\n",
      "Epoch 18: loss=0.0621\n",
      "Epoch loop entered 18\n",
      "Epoch 19: loss=0.0465\n",
      "Epoch loop entered 19\n",
      "Epoch 20: loss=0.0397\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = FetalDataset(\"training_set/training_set_pixel_size_and_HC.csv\")\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    print(\"Epoch loop entered\", epoch)\n",
    "    total_loss = 0\n",
    "    for imgs, masks, _, _ in loader:\n",
    "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = loss_fn(preds, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss={total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d618b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"unet_hc.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dc850",
   "metadata": {},
   "source": [
    "function to compute circumference of ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ede74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, cv2\n",
    "\n",
    "def hc_from_pred(pred_mask, pixel_size):\n",
    "    mask = (pred_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "    cnts,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    if not cnts:\n",
    "        return None\n",
    "\n",
    "    cnt = max(cnts, key=cv2.contourArea)\n",
    "    if len(cnt) < 5:\n",
    "        return None\n",
    "\n",
    "    (_, _), (maj, min_), _ = cv2.fitEllipse(cnt)\n",
    "    a, b = maj/2, min_/2\n",
    "\n",
    "    hc_px = math.pi * (3*(a+b) - math.sqrt((3*a+b)*(a+3*b)))\n",
    "    return hc_px * pixel_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c6496",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d92ac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img, _, px, hc_gt \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# đọc ảnh gốc\u001b[39;00m\n\u001b[32m      9\u001b[39m     orig_img = cv2.imread(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUSTH\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mMLmed\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtraining_set\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     orig_h, orig_w = \u001b[43morig_img\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# preprocess cho model\u001b[39;00m\n\u001b[32m     13\u001b[39m     img = cv2.resize(orig_img, (\u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, _, px, hc_gt in dataset:\n",
    "        # read original image\n",
    "        orig_img = cv2.imread(r\"D:\\USTH\\MLmed\\training_set\\images\", 0)\n",
    "        orig_h, orig_w = orig_img.shape\n",
    "\n",
    "        # preprocess\n",
    "        img = cv2.resize(orig_img, (256, 256))\n",
    "        img = img / 255.0\n",
    "        img = np.stack([img]*3, axis=0)\n",
    "        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # predict\n",
    "        pred = model(img.to(DEVICE))\n",
    "        pred = torch.sigmoid(pred)[0, 0].cpu().numpy()\n",
    "\n",
    "        # resize mask to original size\n",
    "        pred = cv2.resize(\n",
    "            pred,\n",
    "            (orig_w, orig_h),\n",
    "            interpolation=cv2.INTER_LINEAR\n",
    "        )\n",
    "\n",
    "        # compute HC (original pixel_size)\n",
    "        hc = hc_from_pred(pred, px)\n",
    "        if hc is not None:\n",
    "            y_true.append(hc_gt)\n",
    "            y_pred.append(hc)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(\"MAE  (mm):\", mean_absolute_error(y_true, y_pred))\n",
    "print(\"MSE (mm²):\", mean_squared_error(y_true, y_pred))\n",
    "print(\"R²       :\", r2_score(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
